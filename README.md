Efficient Low-Level Medical Question Answering (NLP Project)

This project implements a Quantization + Low-Rank Adaptation (QLoRA) pipeline for fine-tuning a lightweight large language model (LLM) on the MedQA dataset. The goal is to train a parameter-efficient, domain-adapted model capable of answering medical examination-style questions under strict hardware constraints (single GPU).
